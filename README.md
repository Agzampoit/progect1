
Задачи классификации
 =======================
 Метрические алгоритмы классификации 
------------------------------------------------------------------------

**Метрические методы обучения** - методы, основанные на анализе сходства объектов. Для формализации понятия сходства вводится функция расстояния между объектами ![](http://latex.codecogs.com/gif.latex?p(x_1,x_2)). Чем меньше расстояние между объектами, тем больше объекты похожи друг на друга. Метрические классификаторы опираются на гипотезу компактности, которая предполагает, что *схожим объектам чаще соответствуют схожие ответы*.

**Опр 1.1.** Метрические алгоритмы классификации с обучающей выборкой ![](https://latex.codecogs.com/gif.latex?%5Cinline%20X%5E%7Bl%7D) относят классифицируемый объект *u* к тому классу *y*, для которого суммарный вес ближайших обучающих объектов максимален.  

Алгоритм k ближайших соседей (kNN)
-------------------------------------
Данный алгоритм классификации относит классифицируемый объект *z* к тому классу *y*, к которому относится большинство из *k* его ближайших соседей.    
Имеется некоторая выборка ![](https://latex.codecogs.com/gif.latex?%5Cinline%20X%5E%7Bl%7D), состоящая из объектов *x(i), i = 1, ..., l* (например, выборка ирисов Фишера), и класифицируемый объект, который обозначим *z*. Чтобы посчитать расстояние от классифицируемого объекта до остальных точек, нужно использовать функцию расстояния(Евклидово расстояние).  
Далее отсортируем объекты согласно посчитаного расстояния до объекта *z*:  
```diff
 sortObjectsByDist <- function(xl, z, metricFunction = euclideanDistance) ## задаем функцию расстояния
{
distances <- matrix(NA, l, 2)## задаем матрицу расстояния
for (i in 1:l)
{
distances[i, ] <- c(i, metricFunction(xl[i, 1:n], z)) ## считаем расстояние от классифицируемой точки до остальных точек выборки
}

orderedXl <- xl[order(distances[, 2]), ] ##сортируем согласно посчитаного расстояния
return (orderedXl);
}
```
Применяем сам метод kNN, то есть создаем функцию, которая сортирует выборку согласно нашего классифицируемого объекта *z* и относит его к классу ближайших соседей:
```diff
kNN <- function(xl, z, k)
{

orderedXl <- sortObjectsByDist(xl, z) ## Сортируем выборку согласно классифицируемого объекта
n <- dim(orderedXl)[2] - 1

classes <- orderedXl[1:k, n + 1] ## Получаем классы первых k соседей
counts <- table(classes) ## Составляем таблицу встречаемости каждого класса
class <- names(which.max(counts)) ## Находим класс, который доминирует среди первых соседей
return (class) ## возвращаем класс
}

```
 В конце задаем точку *z*, которую нужно классфицировать(ее координаты, выборку и тд).  

Ниже представлен итог работы алгоритма при *k=6*.  
![knn](https://user-images.githubusercontent.com/43229815/47304893-94eaf300-d630-11e8-877f-7b6791c5117d.png)  

**Скользящий контроль(leave-one-out)**  
  
Скользящий контроль, или кросс-валидация (cross-validation, CV) — процедура эмпирического оценивания обобщающей способности алгоритмов, обучаемых по прецедентам.

Фиксируется некоторое множество разбиений исходной выборки на две подвыборки: обучающую и контрольную. Для каждого разбиения выполняется настройка алгоритма по обучающей подвыборке, затем оценивается его средняя ошибка на объектах контрольной подвыборки. Оценкой скользящего контроля называется средняя по всем разбиениям величина ошибки на контрольных подвыборках.  
LeaveOneOut (или LOO) - простая перекрестная проверка.

![](https://latex.codecogs.com/gif.latex?LOO%28k%2C%20X%5El%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bl%7D%20%5Ba%28x_i%3B%20X%5El%20%5Csetminus%20%5C%7Bx_i%5C%7D%2C%20k%29%5Cneq%20y_i%5D%20%5Crightarrow%20%5Cmin)    
Если классифицируемый объект xi не исключать из обучающей выборки, то ближайшим соседом xi всегда будет сам xi, и минимальное (нулевое) значение функционала LOO(k) будет достигаться при k = 1.  

*График зависимости LOO от k=6*:  
![loo](https://user-images.githubusercontent.com/43229815/47098458-efa0db00-d23b-11e8-94ab-1135d9d4020e.png)  

k оптимальное (k_opt) достигается при минимальном LOO (k оптимальное, если оно равно 6).  

Алгоритм 1NN
-----------------------------------
Алгоритм ближайшего соседа(1NN) является самым простым алгоритмом клссификации.  
Данный алгоритм классификации относит классифицируемый объект u к тому классу y, к которому относится его ближайший сосед.

Имеется некоторая выборка Xl, состоящая из объектов x(i), i = 1, ..., l (снова выборка ирисов Фишера). Как и в задаче kNN, задаем функцию расстояния, считаем расстояния от классифицируемой точки до остальных точек выборки, сортируем эти расстояния.  


Далее применяем метод 1NN и классифицируем данный объект, то есть найти ближайший объект выборки и вернуть класс, к которому принадлежит наш объект:
```diff
# применяем метод 1NN
NN1 <- function(xl, point) {	  
	 orderedXl <- sortObjectsByDist(xl, point) # сортировка выборки согласно классифицируемого объекта    
	 n <- dim(orderedXl)[2] - 1 
	 class <- orderedXl[1, n + 1] # получение класса соседа
	 return (class) # возвращаем класс
}

for(i in OX){
	for(j in OY){
	point<-c(i,j)
	class <- NN1(xl, point) 
	points(point[1], point[2], pch = 21, col = colors[class], asp = 1) } # классификация заданного объекта
}
```
Ниже представлена картинка работы данного алгоритма:
![1nn](https://user-images.githubusercontent.com/43229815/47304637-db8c1d80-d62f-11e8-9480-201f1898a68f.png) 

Алгоритм k взвешенных ближайших соседей (wkNN)
----------------------------------------------------------------------  
Имеется некоторая выборка Xl, состоящая из объектов x(i), i = 1, ..., l (в приложенной программе используется выборка ирисов Фишера). Данный алгоритм классификации относит объект u к тому классу y, у которого максимальна сумма весов w_i его ближайших k соседей x(u_i).  
Для оценки близости классифицируемого объекта u к классу y алгоритм wkNN использует следующую функцию:  
![](https://latex.codecogs.com/gif.latex?W%28i%2Cu%29%3D%5Bi%5Cleqslant%20k%5Dw%28i%29),  
 где i -- порядок соседа по расстоянию к классифицируемому объекту u, а w(i) -- строго убывающая функция веса, задаёт вклад i-го соседа в классификацию.  
 

