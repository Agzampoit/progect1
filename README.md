
Задачи классификации
 =======================
 Метрические алгоритмы классификации 
------------------------------------------------------------------------

**Метрические методы обучения** - методы, основанные на анализе сходства объектов. Для формализации понятия сходства вводится функция расстояния между объектами ![](https://latex.codecogs.com/gif.latex?%5Cinline%20p%28u%2C%20x_%7Bu%7D%29). Чем меньше расстояние между объектами, тем больше объекты похожи друг на друга. Метрические классификаторы опираются на гипотезу компактности, которая предполагает, что *схожим объектам чаще соответствуют схожие ответы*.

**Опр 1.1.** Метрические алгоритмы классификации с обучающей выборкой ![](https://latex.codecogs.com/gif.latex?%5Cinline%20X%5E%7Bl%7D) относят объект *u* к тому классу *y*, для которого суммарный вес ближайших обучающих объектов максимален:  
![](https://latex.codecogs.com/gif.latex?a%28u%2C%20X%5E%7Bl%7D%29%3D%20%5Carg%20%5Cmax%20%5CGamma%20_%7By%7D%28u%2C%20X%5E%7Bl%7D%29);  
![](https://latex.codecogs.com/gif.latex?%5CGamma%20_%7By%7D%28u%2C%20X%5E%7Bl%7D%29%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bl%7D%5By_%7Bu%7D%5E%7B%28i%29%7D%3D%20y%5D%5Comega%20%28i%2Cu%29),  
где *w(u,i)*-весовая функция, которая оценивает степень важности *i*-го соседа для классификации объекта *u*. Функция ![](https://latex.codecogs.com/gif.latex?%5Cinline%20%5CGamma%20_%7By%7D%28u%2C%20X%5E%7Bl%7D%29) называется оценкой близости объекта *u* к классу *y*. Выбирая различную весовую функцию *w(i, u)* можно получать различные метрические классификаторы.


 Алгоритм k ближайших соседей (kNN)
-------------------------------------
Имеется некоторая выборка ![](https://latex.codecogs.com/gif.latex?%5Cinline%20X%5E%7Bl%7D), состоящая из объектов *x(i), i = 1, ..., l* (допустим, выборка ирисов Фишера, в нашем случае).   
Данный алгоритм классификации относит классифицируемый объект *u* к тому классу *y*, к которому относится большинство из *k* его ближайших соседей.
Алгоритм относит объект *u* к тому классу, который наберёт большее число голосов:  
![](https://latex.codecogs.com/gif.latex?%5Cinline%20a%28u%2C%20X%5E%7Bl%7D%2C%20k%29%3D%20%5Carg%20%5Cmax%20%5Csum_%7Bi%3D1%7D%5E%7Bk%7D%5By_%7Bu%7D%5E%7B%28i%29%7D%3D%20y%5D),  

***Реализация алгоритма: kNN***

(вставить скрины)

### Преимущества:

1. Простота реализации и возможность введения различных модификаций весовой функции *w(i,u)*.
2. При k, подобранном около оптимального, алгоритм "неплохо" классифицирует.
3. "Прецедентная" логика работы алгоритма, хорошо понятна экспертам в таких предметных областях, как медицина, биометрия, юриспруденция и др.

### Недостатки:

1. Приходится хранить всю обучающую выборку целиком.
2. При k = 1 неустойчивость к погрешностям (выбросам -- объектам, которые окружены объектами чужого класса), 
вследствие чего этот выброс классифицировался неверно и окружающие его объекты, для которого он окажется ближайшим, тоже.
3. При k = l алгоритм наоборот чрезмерно устойчив и вырождается в константу.
4. Максимальная сумма объектов в counts может достигаться в нескольких классах одновременно.
5. "Скудный" набор параметров.
6. Точки, расстояние между которыми одинаково, не все будут учитываться.

Алгоритм 1NN
-----------------------------------
Алгоритм ближайшего соседа(1NN) является самым простым алгоритмом клссификации.  
Данный алгоритм классификации относит классифицируемый объект u к тому классу y, к которому относится его ближайший сосед.
Единственное достоинство этого алгоритма - простота реализации.

### Недостатков больше:
-----------
1. Данный алгорритм неустойчив к погрешностям. Если среди обучающих объеков есть выброс - объект, находящийся в окружении объектов чужого класса, 
то не только он сам будет классифицирован неверно, но те окружающие его объекты, для которых он окажется ближайшим, также будут класифицированы неверно.
2. Отсутсвие параметров, которые можно было бы настраивать по выборке.
Алгоритм полностью зависит от того, насколько удачно выбрана метрика p(расстояния).
3. В результате - низкое качество классификации.

Описание кода:
Имеется некоторая выборка Xl, состоящая из объектов x(i), i = 1, ..., l (снова выборка ирисов Фишера). 
Далее необходимо задать функцию расстояния, которая реализуется следующим образом:  
'''diff
jfknhd
'''

 
Затем сортируем объекты выборки согласно заданной функции расстояния. 
Создаем матрицу расстояний, где находим
расстояния от классифицируемой точки до остальных точек.

После проделанного пишем саму функцию 1NN.  
Здесь найти ближайший объект выборки и в конечном итоге вернуть класс, к которому принадлежит наш объект.  
